# 一、概述

## 1.1 什么是机器学习

Arthur Samuel 对机器学习的定义

​	Machine Learning is Fields of study that gives computers the ability to learn without being explicitly programmed

​	机器学习是这样的领域，它赋予计算机学习的能力，（这种学习能力）不是通过**显著式编程**获得的

非显著式编程：让计算机自己总结规律的编程方法

Tom Mitshell 对机器学习的定义

​	A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its perfomance on T, as measured by P, improves with experience E.

​	一个计算机程序被称为可以学习，是指它能够针对一个任务 T 和某个性能指标 P，从经验 E 中学习。这种学习的特点是，它在 T 上的被 P 所衡量的性能，会随着经验 E 的增加而提高。

例如：识别菊花和玫瑰

任务 T：编写计算机程序识别菊花和玫瑰

经验 E：一大堆菊花和玫瑰图片

指标 P：识别率

例如：让机器人冲咖啡

任务 T：设计程序让机器人冲咖啡

经验 E：机器人多次尝试的行为和这些行为产生的结果（摔倒，撞墙，冲到咖啡）

指标 P：在规定时间内成功冲到咖啡的次数

Tom Mitshell 对机器学习的定义更加数学化，根据经验 E 提高性能指标 P 的过程就是数学的**最优化**过程。

>  思考题
>
> 1. 教计算机下棋
> 2. 垃圾邮件识别，教计算机自动识别某个邮件是否是垃圾邮件
> 3. 人脸识别，教计算机通过人脸图像识别这个人是谁
> 4. 无人驾驶，教计算机自动驾驶汽车从一个点到另一个点
>
> 这四个任务的经验 E 和指标 P是什么？

## 1.2 机器学习分类

上述思考题四个任务，可以分成两类，那哪两个是一类？[1,4] 和 [2,3]，分类的依据是什么？

答案是：按照经验 E，我们发现2，3 我们需要搜集大量的邮件(人脸图片)并告诉计算机哪个是垃圾邮件哪个不是(每张照片是谁)，这个过程我们称为**为训练数据打标签**(Labeling for training data)，2，3任务的 E 就是**训练样本**和**标签**对应的集合。总结：[2,3]这类任务所有的经验 E 都是人工采集并输入计算机的，称为：**监督学习**，[1,4]这类任务的经验 E 是由计算机与环境互动获得的，计算机产生行为，并获的这些行为产生的结果，程序只需要定义这些行为的**收益函数**(Reward function)，对行为进行奖励或成本；我们可以设计算法让计算机改变自己的行为模式去**最大化**收益函数，称为：**强化学习**(计算机通过与环境互动逐渐强化自己的行为模式)。

### 1.2.1 监督学习

监督学习根据数据标签存在与否分类：

1. 传统的监督学习(Traditional Supervised Learning)
   1. 所有的训练数据都有对应的标签
   2. 算法：支持向量机、人工神经网络、深度神经网络等
2. 非监督学习(Unsupervised Learning)
   1. 所有的训练数据都没有对应的标签
   2. 算法：聚类、EM算法、主成分分析等
3. 半监督学习(Semi-supervised Learning)
   1. 训练数据中一部分有标签一部分没有标签

监督学习根据标签的连续离散分类：

1. 分类：标签是离散的
2. 回归：标签是连续的

注：连续和离散的定义是非常模糊的，因此分类问题和回归问题的界限也是非常模糊的

> 思考题
>
> 1. 除了上述标准，是否还能提出其他的分类标准？
> 2. 提出一个机器学习不属于上述的分类

## 1.3 机器学习算法的过程

第一步：特征提取

观察训练数据，通过训练数据样本获得对机器学习任务有帮助的多维读数据

注：机器学习的重点不是学习如何提取特征，而是假设在已经提取好特征的前提下，如何构建算法获得更好的性能指标，但是实践表明，好的特征往往会获得不错的性能。

第二步：特征选择

基于特征提取，观察提取出来的特征能否很好的区分训练数据

第三步：选择算法

## 1.4 算法的选择

机器学习算法过程的第三步：选择算法，即：哪种机器学习的算法最好

1995 年 D.H.Wopert 等人提出：**没有免费午餐定理**(No Free Lunch Theorem)

任何一个预测函数，如果在一些训练样本上表现好，那么必然在另一些训练样本上表现不好，如果不对数据在特征空间的先验分布有一定假设，那么表现好与表现不好的情况一样多

例如：我们用机器学习算法预测抛硬币，无论用哪种算法其预测结果的正确率都是 50%，问题出在**假设各种情况出现的先验概率是一样的**。

但实际情况不应该是这样

<img src="https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213103629230.png" alt="image-20211213103629230" style="zoom:50%;" />

所有的机器学习算法都会将上面的 ？预测为 O，下面的 ？预测为 X，这是因为我们在设计机器学习算法的时候有一个假设：**在特征空间上距离接近的样本，他们属于同一个类别的概率会更高**，基于这个假设每个格子的先验概率是不一样。这样的假设有道理吗？这是属于哲学问题。

总结：**不对特征空间的先验分布有假设，所有的算法表现都一样**

该定理告诉我们：再好的算法也存在犯错的风险；没有放之四海而皆准的最好算法

机器学习的本质：在有限已知的数据，在复杂的高维度特征空间中预测未知的样本

# 二、支持向量机

支持向量机(Support Vector Machine) - Vladimir Vapnik[俄]

## 2.1 前置概念

线性可分(Linear Separable)：存在一条直线，可以完全分开样本数据

线性不可分(Nonlinear Separable)：不存在一个直线，可以分开样本数据

注：二维特征空间为直线，三维为平面，高维人脑不可感知

数学定义：

<img src="https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213113913586.png" alt="image-20211213113913586" style="zoom:50%;" />

一个训练样本集 $\{(X_1,y_1),(X_2,y_2),...,(X_n,y_n)\}$ 其中 $X_i=(x_{i1},x_{i2})^T, y_i=\{+1,-1\}$ 线性可分，指的是存在 $(w_1,w_2,b)$ 有：
$$
\begin{cases}
w_1x_{i1}+w_2x_{i2}+b > 0，y_i=+1\\
\\
w_1x_{i1}+w_2x_{i2}+b < 0，y_i=-1
\end{cases}
$$
推广至高维有

若一个训练样本集 $\{(X_1,y_1),(X_2,y_2),...,(X_n,y_n)\}$，其中 $X_i=(x_{i1},x_{i2},...x_{im})^T，y_i=\{+1,-1\}$线性可分，则存在 $W=(w_1,w_2,...,w_m)^T$和 b，有
$$
\begin{cases}
W^TX_i+b > 0，y_i=+1\\
\\
W^TX_i+b < 0，y_i=-1
\end{cases}
$$
对于 y 的取值是人为定义的，比如 y=1 代表是圆，y=-1 代表是菱形

> 思考题
>
> 1. 现实生活中，哪些是线性可分，哪些是线性不可分
> 2. 上述只给出二分类线性可分的定义，给出类别数大于2的线性可分严格数学定义
> 3. 证明：在二分类情况下，如果一个数据集线性可分，即存在一个超平面将两个类别完全分开，那么一定存在无数多个超平面将两个类别完全分开

## 2.2 问题描述

从上面的定理：**如果一个数据集线性可分，即存在一个超平面将两个类别完全分开，那么一定存在无数多个超平面将两个类别完全分开** 出发。

Vapnik 的支持向量机算法主要为两个步骤

1. 解决线性可分问题
2. 再将线性可分问题中获得的结论推广到线性不可分的情况

在解决线性可分的问题中，Vapnik 问了一个问题，**在这无数多个超平面中，到底哪一个最好**？

在二维特征空间中的二分类问题，有如下三条直线将圆形和菱形分开

<img src="https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213130401682.png" alt="image-20211213130401682" style="zoom:50%;" />

你认为1，2，3哪条直线最好？答案似乎都是2。但根据没有免费午餐定理，在没有假设特征空间的先验分布情况下，三条线的表现是一样的，那为什么会任务直线 2 的表现更好一点呢？实际上我们不自然的对特征空间的先验分布做了一定的假设。例如：

假设训练样本的位置在特征空间上有测量误差，误差成正态分布。

<img src="https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213131033324.png" alt="image-20211213131033324" style="zoom:50%;" />

假设菱形的实际位置在它的上方，那么使用直线 3 就会产生错误的分类，同理直线 1 也是，也就是说：直线 2 对训练样本所处的位置的误差容忍程度是最高的。

**那么直线 2 是怎么画出来的？**

首先对于任意一条可以将数据集完全的分开的直线，将其上下平移知道直线接触到数据集的样本，如图

<img src="https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213141205617.png" alt="image-20211213141205617" style="zoom:50%;" />

我们将直线接触到的样本集称为支持向量，将这两条平行线的距离称为间隔(Margin)，因此选择直线 2 的依据是其间隔 Margin 最大

![image-20211213142406407](https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213142406407.png)

事实上仅使用 Margin 最大这一个条件并不能唯一的确定一条直线(只确定了斜率)，为了唯一确定一条直线，规定这条直线处于两个平行线中间的位置。

因此，支持向量机寻找的最优分类直线应满足：

1. 该直线分开了两类
2. 该直线的间隔最大
3. 该直线处于间隔的中间，到所有支持向量距离相等

上述是基于二维空间的二分类，但对于高维空间同样适用

> 思考题
>
> 证明：在线性可分的情况下，有且只有唯一一条直线满足上面三个条件。

## 2.3 优化问题

从上节的定理延伸到高维

在数据集线性可分的情况下，支持向量机寻找的最优分类超平面应该满足：

1. 该超平面分开了两个类别
2. 该超平面有最大的间隔
3. 该超平面处于间隔的中间，到所有支持向量距离相等

下面将证明寻找最优分类超平面的过程 <=> 最优化过程

根据线性可分的定义得知，存在一个超平面 $w^Tx_i+b=0$，把数据集一分为二，对于数据集上任意一点$x_0$，若在超平面上面则 $w^Tx_0+b>0$，定义为正例，反之负例有 $w^Tx_0+b<0$，正例可以指定标签 y=1，负例指定标签 y=-1，因此用 $y_i(w^Tx_i+b)>0$ 可以很好的定义二分类的线性可分。

在证明前需要明确几个定理

定理一：

$w^Tx_i+b$ 和 $aw^Tx_i+ab$ 是一个超平面

定理二：

一个点 $x_0$ 到超平面 $w^Tx_i+b$ 的距离
$$
d=\frac {|w^Tx_0+b|}{||w||}
$$
根据定理一，最优超平面条件三可以使用 $a$ 来缩放 $(w,b)$ 使之满足

在支持向量 $x_0$ 上 $|w^Tx_0+b|=1$

在非支持向量 $x_0$ 上 $|w^Tx_0+b|>1$

若要满足超平面有最大的间隔，且到所有支持向量的距离相等也就是
$$
maxd=\frac{1}{||w||} <=> min||w|| <=> min\frac{1}{2}||w||^2
$$
即目标函数，那它的限制条件就是 $|w^Tx_+b| \geq 1$，将标记 y 引入则 $y_i(w^Tx_i+b) \geq 1$

因此寻找最优超平面的过程可以转换为一个最优化问题：
$$
min_{w,b} \frac{1}{2}||w||^2
\\\\
st. \ y_i(w^Tx_i+b) \geq 1 \ ,i=1...n
$$
且为一个二次规划凸优化问题，$x_i,y_i$ 是已知的，且目标函数是二次项，限制条件是一次项，这类问题要么无解，要么只有唯一的最小值解。

> 思考题
>
> 支持向量机的限制条件如果从大于等于 1 变成大于等于 2，则 (w,b) 会变成 ($a$w,$a$b)，问：如果$x_i$和 w 是m维向量，求 $a$

## 2.4 线性不可分情况

在训练集线性不可分的情况下，上述的最优化问题是**无解**的。

<img src="https://gitee.com/uhope/oos/raw/master/2021-12-13/image-20211213163219836.png" alt="image-20211213163219836" style="zoom:50%;" />

如图是无法通过一个超平面将训练集分开的，也就是说上述的不等式约束并不都成立，如果修改不等式的约束条件让不满足的数据满足，称为**软间隔最大化**(相对原先训练集所有数据都要满足的硬间隔最大化)。

假设现在的训练集是线性不可分的，通常情况下，训练集存在一些奇异点(outlier)，如原先在超平面的上方，y = 1，因为线性不可分，这个点现在再超平面下方导致代入超平面求得的值与分类决策函数 y 符号相反，也就是说这类奇异点在 $y_i(w^Tx_i+b)$ 是负数，为了满足间隔大于等于 1 这个约束条件，针对每个样本点引入松弛变量 $\xi_i\ge0$,加上这个松弛变量使其满足约束条件，这样约束条件就变为：
$$
y_i(w^Tx_i+b)+\xi_i \geq 1 => y_i(w^Tx_i+b) \geq 1 - \xi_i
$$
同时对每个松弛变量都需要支付一个代价 $\xi_i$ 目标函数则变为：
$$
\frac{1}{2}||w||^2+C\sum_{i=1}^n\xi_i
$$
这里 C > 0，是一个惩罚参数，是模型的超参数，这类参数由应用程序提供，C 越大表示对错误分类的惩罚越大，若 C 趋于无穷表示允许错误分类，也就变成了线性可分问题了。上述最小化目标函数包含两层含义：使 $\frac{1}{2}||w||^2$ 尽可能小即间距尽可能大；同时使错误分类的点尽量小，这样才能整体求最小。

因此线性不可分的问题变成了凸二次规划问题
$$
min_{w,b,\xi} \frac{1}{2}||w||^2+C\sum_{i=1}^n\xi_i
\\\\
st. \ y_i(w^Tx_i+b) \geq 1-\xi_i \ ,i=1...n \\\
\xi_i \ge 0 \ ,i=1...n
$$
上述问题可证明是存在解的，通过尝试不同的 C，来获取最高识别率，但求解的结果依然是一个超平面，其结果不被我们所满意，其本质是无法使用线性模型来分开线性不可分的数据集，因此必须扩大可选的函数范围。

> 思考题
>
> 针对上述的线性不可分训练集，是否对训练集做非线性变换，使其变换后的训练集满足线性可分呢？

## 2.5 扩大可选函数范围

